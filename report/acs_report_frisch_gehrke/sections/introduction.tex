\documentclass[../main/main.tex]{subfiles}

% Put everything that shall appear in the introduction
% inside this document environment.
\begin{document}
	The question of how to predict peoples performance on a task has been of great interest in the psychology literature. To be able to make qualified statements about this ability, one has to define it first.
	This can be achieved by the framework of \textit{Metacognition} including the frequently used terms \textit{Metacognitive Sensitivity}, \textit{Metacognitive Bias} and \textit{Metacognitive Efficiency}, as presented by \citep{fleming2014measure}, that will be explained during this first section.\\
	\textit{Metacognitive Sensitivity} is used to express how good a subject is at differing between his or her own correct and incorrect answers. For example, imagine a classical experiment from \textit{Signal Detection Theory (SDT)} where the subject's task is to rate a stimulus to come from a class A or a distinct class B. Poor metacognitive sensitivity would result in bad judgments about whether the subject chose the correct answer, even though the classes are easy to separate for an average observer. A useful initial approach to quantify \textit{Metacognitive Sensitivity} is the $2x2$ confidence-accuracy table, labeled \textit{Type-2 SDT Table} by \citep{fleming2014measure}, that is the equivalent of the usual \textit{Type-1 SDT Table} \citep{cramer1999mathematical}, shown in the upper part from figure \ref{fig:tables}, applied to the \textit{Metacognition} framework. The resulting table is displayed in the lower part of figure \ref{fig:tables}. Typical SDT measurements of the association between the rows and the columns of the table in the type-1 case are the \textit{$\phi$-Correlation}, defined e.g. in \citep{cramer1999mathematical}, and the \textit{Goodman-Kruskall Gamma Coefficient $G$} from \citep{goodman1963measures}. Imagine a subject reporting a subjective confidence for multiple trials of the former example stored in a vector, e.g $(A, B, B, A)$, and the vector of the actual correct classifications $(A, B, A, B)$. If we encode $A=1$ and $B=0$, the $\phi$-correlation is the \textit{Pearson r-Correlation} between both vectors. The advantage of using $G$ instead is the abundance of any distributional assumptions on the data and the possibility to easily extend the measurement to a confidence rating scale (e.g. from 0 to 100), rather than a binary encoding (e.g. 0/1). Nevertheless it is well known that both measurements are affected by bias \citep{fleming2014measure}. Both, \citep{nelson1984comparison} and \citep{masson2009sources} could show that this also holds for the type-2 application of these measurements.
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.6\textwidth]{../assets/type1_sdt_table.png}
		\includegraphics[width=0.6\textwidth]{../assets/type2_sdt_table.png}
		\caption{The type-1 and type-2 2x2 SDT-tables as explained in 						\citep{fleming2014measure}.} 
		\label{fig:tables}
	\end{figure}
	A standard way to remove the influence of the bias in classic SDT is using $d'$ as in \citep{green1966signal} which will be constant given different biases and also has several approaches to metacognitive sensitivity, e.g. \citep{kunimoto2001confidence} defined type-2 $d'$ as
	\begin{equation}
		d' = z(H2) - z(FA2)
	\end{equation}
	A visualization of this measurement is shown on the left side of figure \ref{fig:d_dash_and_roc}.
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.4\textwidth]{../assets/d_dash.png}
		\includegraphics[width=0.35\textwidth]{../assets/type1_roc.png}
		\caption{The left side of this figure shows an example for the d' measurement, calculated as the difference between the (inverse of) the hit rate (HR) and the false alarm rate (FA). The right side displays an example for a type-2 ROC curve, displaying the relationship between FA2 and HR2. The area under the ROC, called \textit{AUROC} gives a measurement of a subject's metacognitive sensitivity.} 
		\label{fig:d_dash_and_roc}
	\end{figure}
	Despite this advantage, $d'$ can not easily applied to the type-2 data, because it includes assumptions of gaussian distributions with equal variance, which is very unlikely to be the case in type-2 data, as shown by \citep{galvin2003type}. Experiments of \citep{azzopardi2007evaluation} showed that this measurement is indeed affected by changes in the metacognitive bias.\\
	One way to remove this issue is the use of non-parametric analysis, that does not make these assumptions, e.g. the \textit{Receiver Operating Characteristic (ROC)} analysis [x] in classic SDT. This approach can be applied to type-2 data analogue to \citep{clarke1959two} and an example is given in the right part of figure \ref{fig:d_dash_and_roc}.\\
	A further complication for using the above methods to measure metacognitive sensitivity is the fact that all these measures are affected by the task performance \citep{galvin2003type}. This can be addressed by explicitly modeling the connection between a subject's performance and metacognition in a model-based approach. The \textit{Meta-$d'$} measure, defined by \citep{maniscalco2012signal}, makes use of the fact that given gaussian variance assumptions at type-1 level, the shapes of the type-2 distributions are known even if they are not themselves gaussian. Therefore the optimal type-2 performance is constrained by one's type-1 performance. E.g. given a particular type-1 variance structure and bias, the form of the type-2 ROC can be completely determined. So, given a subject's actual type-2 performance, one could reconstruct the underlying type-1 sensitivity, labeled meta-$d'$, that is robust to changes in the bias and recovers simulated changes in metacognitive sensitivity, e.g. by altering task difficulties. For a metacognitive ideal observer, meta-$d'$ should be equal to $d'$. To measure this quantity, \citep{fleming2014measure} defined \textit{Metacognitive Efficiency} as the ratio of meta-$d'/d'$, or by the more stable variants meta-$d'-d'$ or the logarithm of meta-$d'/d'$. However, this measurement is unable to discriminate between different causes of a change in metacognitive efficiency. For example,  trial-to-trial variability in the placement of confidence criteria results in decreasing efficiency as well as additional noise in the evidence used to make the confidence rating. A similar bias-free approach to model metacognitive accuracy is the \textit{Stochastic Detection and Retrieval Model (SDRM)} from \citep{jang2012stochastic} which we do not want to cover here.\\
	A somewhat different approach uses so-called \textit{One-Shot} discrepancy measures to quantify metacognition, e.g. in \citep{kruger1999unskilled}. A general confidence rating (asked before of after the trials) is compared to the actual performance on a variety of tasks, but it should be clear from the above that using a single rating of performance will not result in a good distinction between the bias and sensitivity, nor will it enable to measure the efficiency \citep{fleming2014measure}. In contrast, collecting trial-by-trial measures of performance and metacognitive judgments allows to get a picture of an individuals bias, sensitivity and efficiency.\\
	To get a different view-point on the domain, one could formalize metacognitive confidence as the ability to make good probability judgments directed towards the accuracy of one's own actions.
	
	\subsection{Formalizing metacognition as probability judgments}
	
	In this framework \textit{Metacognition} gets a normative interpretation as the accuracy of a probability judgment about one's own performance and one advantage is the possibility to elicit a meaningful measure of the bias. A lot of literature is a available on how to measure this accuracy, but in the following we want to focus on one of them, called the \textit{Brier Score}. To define this score, \citep{fleming2014measure} first defined the \textit{Probability Score (PS)}, analogue to \citep{harvey1997confidence}, as the squared difference between a probability rating $f$ for an event, and it's actual occurrence $c$ (0 or 1 for binary events):
	\begin{equation}
			PS = (f - c)^2
	\end{equation}
	Based on this, the \textit{Brier Score (BS)} from \citep{brier1950verification} can then be defined as the mean value of the PS averaged across all estimates $i$:
	\begin{equation}
			BS = \frac{1}{N}\sum_i(f_i - c_i)^2
	\end{equation}
	This score is an equivalent of the $\phi$ or $G$ measurements explained above and can be decomposed, as shown by \citep{murphy1973new}, in the following way:
	\begin{equation}
		BS = O + C - R
	\end{equation}
	where $O$ is the \textit{Outcome Index}, reflecting the variance of the outcome event $c$:
	\begin{equation}
		O = \bar{c}(1-\bar{c})
	\end{equation}
	The \textit{Calibration} expresses the goodness of fit between the probability assessments and the corresponding proportion of correct responses. It quantifies the discrepancy between the mean performance level in a category (e.g. $60\%$) and its associated rating (e.g. $80\%$):
	\begin{equation}
		C = \frac{1}{N}\sum_{j=1}^{N}N_j(f_j - \bar{c_j})
	\end{equation}
	Last, the \textit{Resolution} $R$ encodes the variance of the probability assessments, measuring the extent to which correct and incorrect answers are assigned to different probability categories:
	\begin{equation}
		R = \frac{1}{N}\sum_{j=1}^{N}N_j(\bar{c_j} - \bar{c})
	\end{equation}
	As mentioned earlier, this framework allows us to extract more meaningful quantities about a subject's metacognitive sensitivity. One such quantity is the calibration of a subject, as shown in figure \ref{fig:calibration}, that directly shows over- or underconfident tendencies of a person.
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.5\textwidth]{../assets/calibration_curve.png}
		\caption{Example of one over- and one underconfident calibration curve.} 
		\label{fig:calibration}
	\end{figure}
	For multiple possible outcome classes, the brier score can be calculated as shown by \citep{brier1950verification} by summing over all classes $j$:
	\begin{equation}
		\label{eq:brier_score}
		BS = \frac{1}{N}\sum_j\sum_i(f_{ij} - c_{ij})^2
	\end{equation}
	Note that this formulation does have a range of scores from 0.0 to 2.0, where the analogue values of the one-class formulation are half as high, e.g. a brier score of 0.5 in the one-class case would correspond to a score of 1.0 in the multi-class formulation.\\
	The next chapter gives an explanaiton of our methods to actually capture a subjects confidence and calculate a measure of the brier score out of it. We present an experiment that we evaluated on several subjects for different tasks, before we display the results of our measurements in chapter \ref{sec:results} and finally come to a conclusion on our attempt to measure metacognition in chapter \ref{sec:discussion}.
	

\end{document}